{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch05 Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추천시스템의 최종 목표는 각 사용자의 각 아이템에 대한 선호도(구매 확률)를 예측하는 것이다.  \n",
    "Factorization Machines(FM)은 사용자와 아이템의 다양한 특성을 모델화함으로써 예측의 성능을 높이려는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MF 모델은 사용자의 취향과 아이템의 특성으로 각 사용자의 각 아이템의 선호도를 예측하는 방식이다.  \n",
    "그런데 사용자의 취향과 아이템의 특성뿐 아니라 예측에 도움을 줄 수 있는 다른 변수가 존재할 수 있다.  \n",
    "성별, 지역 등 인구통계 변수나 구매 시점 등의 변수를 추가하면 더 정확해지는데 이러한 변수를 종합해서 요인화 해주는 방법이 FM이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 FM의 표준식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM의 기본 아이디어는 모든 변수와 그 변수들 간의 상호작용을 고려하여 평점을 예측하는 것이다.  \n",
    "영화 평점 예측을 예를 들면 모든 변수가 영화의 평점에 영향을 학습하고 이를 바탕으로 평점을 예측하는 모델이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://velog.velcdn.com/images/amzyoungchae/post/16c06119-7446-42db-a516-f7519aa86a1e/image.png' width = 500 height = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y$ : 예측값  \n",
    "$x$ : input variable(입력 변수)  \n",
    "$w_0$ : global bias(전체 편향, 전체 평균)  \n",
    "$w_i$ : bias(입력 변수 $x_i$의 편향)  \n",
    "$v_i$ : latent matrix(잠재 요인 행렬) $v$에서 변수 $x_i$의 특성값  \n",
    "$n$ : n of input variable(입력 변수의 수)  \n",
    "$k$ : n of latent factors(잠재요인의 수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM은 MF에 다른 변수를 추가할 수 있도록 좀 더 일반화한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 FM 식의 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://velog.velcdn.com/images/amzyoungchae/post/0ca848bd-8cb8-4dbd-b21b-75df2b16a897/image.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM 의 표준식은 계산의 효율성이 좋지 않기 때문에 FM을 실제로 적용할 때에는 변형된 식을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 FM의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM 학습은 다른 기계학습 방식과 동일하게 진행되는데 아래와 같은 절차를 따른다.\n",
    "1. 비용함수를 설정한다. (예를 들면 RMSE)\n",
    "2. $w_0, w, v$를 초기화한다.\n",
    "3. 주어진 $w_0, w, v$에 따라 비용함수를 계산한다.\n",
    "4. $w,v$ 의 update rule 에 따라 $w,v$ 를 업데이트한다.\n",
    "5. 비용함수가 더 이상 개선되지 않을 떄까지 3,4 단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 FM의 데이터 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Python으로 FM의 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 변수를 사용하는 예는 뒤에서 설명하고 사용자 id와 아이템 id 만 사용하는 FM의 구현 예를 살펴보자.  \n",
    "데이터를 읽어서 sparse matrix 형태로 변형하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 데이터 읽기\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('./data/u.data', sep='\\t', names=r_cols, encoding='latin-1') \n",
    "\n",
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(ratings['user_id']):\n",
    "    user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(ratings['movie_id']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "num_x = start_point # Total number of x\n",
    "ratings = shuffle(ratings, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding  0  cases...\n",
      "Encoding  10000  cases...\n",
      "Encoding  20000  cases...\n",
      "Encoding  30000  cases...\n",
      "Encoding  40000  cases...\n",
      "Encoding  50000  cases...\n",
      "Encoding  60000  cases...\n",
      "Encoding  70000  cases...\n",
      "Encoding  80000  cases...\n",
      "Encoding  90000  cases...\n"
     ]
    }
   ],
   "source": [
    "# Generate X data\n",
    "data = []\n",
    "y = []\n",
    "w0 = np.mean(ratings['rating'])\n",
    "for i in range(len(ratings)):\n",
    "    case = ratings.iloc[i]\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(user_dict[case['user_id']]) # User id encoding\n",
    "    x_value.append(1)\n",
    "    x_index.append(item_dict[case['movie_id']]) # Movie id encoding\n",
    "    x_value.append(1)\n",
    "    data.append([x_index, x_value])\n",
    "    y.append(case['rating'] - w0)\n",
    "    if (i % 10000) == 0:\n",
    "        print('Encoding ', i, ' cases...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 FM을 구현한 클래스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
    "\n",
    "class FM():\n",
    "    def __init__(self, N, K, data, y, alpha, beta, train_ratio=0.75, iterations=100, tolerance=0.005, l2_reg=True, verbose=True):\n",
    "        self.K = K          # Number of latent factors\n",
    "        self.N = N          # Number of x (variables)\n",
    "        self.n_cases = len(data)            # N of observations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # w 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
    "        # v 초기화\n",
    "        self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
    "        # Train/Test 분리\n",
    "        cutoff = int(train_ratio * len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.test_x = data[cutoff:]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.test_y = y[cutoff:]\n",
    "\n",
    "    def test(self):                                     # Training 하면서 RMSE 계산 \n",
    "        # SGD를 iterations 숫자만큼 수행\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            rmse1 = self.sgd(self.train_x, self.train_y)        # SGD & Train RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x, self.test_y)    # Test RMSE 계산     \n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                       # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance:  # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process\n",
    "        \n",
    "    # w, v 업데이트를 위한 Stochastic gradient descent \n",
    "    def sgd(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            x_idx = data[0]\n",
    "            x_0 = np.array(data[1])     # xi axis=0 [1, 2, 3]\n",
    "            x_1 = x_0.reshape(-1, 1)    # xi axis=1 [[1], [2], [3]]\n",
    "    \n",
    "            # biases\n",
    "            bias_score = np.sum(self.w[x_idx] * x_0)\n",
    "    \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1)          # v matrix * x\n",
    "            sum_vx = np.sum(vx, axis=0)         # sigma(vx)\n",
    "            sum_vx_2 = np.sum(vx * vx, axis=0)  # ( v matrix * x )의 제곱\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score\n",
    "            y_pred.append(y_hat)\n",
    "            error = y - y_hat\n",
    "            # w, v 업데이트\n",
    "            if self.l2_reg:     # regularization이 있는 경우\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "            else:               # regularization이 없는 경우\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1))\n",
    "        return RMSE(y_data, y_pred)\n",
    "\n",
    "    def test_rmse(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data , y in zip(x_data, y_data):\n",
    "            y_hat = self.predict(data[0], data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE(y_data, y_pred)\n",
    "\n",
    "    def predict(self, idx, x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1, 1)\n",
    "\n",
    "        # biases\n",
    "        bias_score = np.sum(self.w[idx] * x_0)\n",
    "\n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx, axis=0)\n",
    "        sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.956651 ; Test RMSE = 0.972614\n",
      "Iteration: 20 ; Train RMSE = 0.935132 ; Test RMSE = 0.956315\n",
      "Iteration: 30 ; Train RMSE = 0.926349 ; Test RMSE = 0.949946\n",
      "Iteration: 40 ; Train RMSE = 0.921554 ; Test RMSE = 0.946727\n",
      "Iteration: 50 ; Train RMSE = 0.918415 ; Test RMSE = 0.944877\n",
      "Iteration: 60 ; Train RMSE = 0.915865 ; Test RMSE = 0.943649\n",
      "Iteration: 70 ; Train RMSE = 0.913073 ; Test RMSE = 0.942575\n",
      "Iteration: 80 ; Train RMSE = 0.908936 ; Test RMSE = 0.941146\n",
      "Iteration: 90 ; Train RMSE = 0.901689 ; Test RMSE = 0.938625\n",
      "Iteration: 100 ; Train RMSE = 0.889230 ; Test RMSE = 0.934301\n",
      "Iteration: 110 ; Train RMSE = 0.871065 ; Test RMSE = 0.928619\n",
      "Iteration: 120 ; Train RMSE = 0.848864 ; Test RMSE = 0.923226\n",
      "Iteration: 130 ; Train RMSE = 0.823247 ; Test RMSE = 0.918859\n",
      "Iteration: 140 ; Train RMSE = 0.793320 ; Test RMSE = 0.915355\n",
      "Iteration: 150 ; Train RMSE = 0.758559 ; Test RMSE = 0.912760\n",
      "Iteration: 160 ; Train RMSE = 0.719489 ; Test RMSE = 0.911336\n",
      "Iteration: 170 ; Train RMSE = 0.677379 ; Test RMSE = 0.911298\n",
      "164 0.9111370169653403\n"
     ]
    }
   ],
   "source": [
    "K = 350\n",
    "fm1 = FM(num_x, K, data, y, alpha = 0.0014, beta = 0.075, train_ratio= 0.75, iterations=300, tolerance=0.0005, l2_reg=True, verbose=True)\n",
    "result = fm1.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 사용자, 아이템 외의 추가 데이터까지 사용하는 경우를 살펴보기로 한다.  \n",
    "MovieLens 에 있는 데이터 중에서 사용자의 occupation, gender, age, genre를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('./data/u.user', sep='|', names=u_cols, encoding='latin-1')\n",
    "\n",
    "i_cols = ['movie_id', 'title', 'release date', 'video release date', 'IMDB URL', \n",
    "          'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', \n",
    "          'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n",
    "          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "movies = pd.read_csv('./data/u.item', sep='|', names=i_cols, encoding='latin-1')\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('./data/u.data', sep='\\t', names=r_cols, encoding='latin-1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(users['user_id']):\n",
    "    user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(movies['movie_id']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "\n",
    "# Occupation encoding\n",
    "occ_dict = {}\n",
    "for i in set(users['occupation']):\n",
    "    occ_dict[i] = start_point + len(occ_dict)\n",
    "n_occ = len(occ_dict)\n",
    "start_point += n_occ\n",
    "\n",
    "# Gender encoding\n",
    "gender_dict = {}\n",
    "for i in set(users['sex']):\n",
    "    gender_dict[i] = start_point + len(gender_dict)\n",
    "n_gender = len(gender_dict)\n",
    "start_point += n_gender\n",
    "\n",
    "# Genre encoding\n",
    "genre_dict = {}\n",
    "genre = ['unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', \n",
    "          'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n",
    "          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "for i in genre:\n",
    "    genre_dict[i] = start_point + len(genre_dict)\n",
    "n_genre = len(genre_dict)\n",
    "start_point += n_genre\n",
    "age_index = start_point\n",
    "start_point += 1\n",
    "num_x = start_point # Total number of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "movies = movies.drop(['title', 'release date', 'video release date', 'IMDB URL'], axis=1)\n",
    "users = users.drop(['zip_code'], axis=1)\n",
    "ratings = ratings.drop(['timestamp'], axis=1)\n",
    "\n",
    "x = pd.merge(ratings, movies, how = 'outer', on = 'movie_id')\n",
    "x = pd.merge(x, users, how = 'outer', on = 'user_id')\n",
    "x = shuffle(x, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding  0  cases...\n",
      "Encoding  10000  cases...\n",
      "Encoding  20000  cases...\n",
      "Encoding  30000  cases...\n",
      "Encoding  40000  cases...\n",
      "Encoding  50000  cases...\n",
      "Encoding  60000  cases...\n",
      "Encoding  70000  cases...\n",
      "Encoding  80000  cases...\n",
      "Encoding  90000  cases...\n"
     ]
    }
   ],
   "source": [
    "# Generate X data\n",
    "data = []\n",
    "y = []\n",
    "age_mean = np.mean(x['age'])\n",
    "age_std = np.std(x['age'])\n",
    "w0 = np.mean(x['rating'])\n",
    "for i in range(len(x)):\n",
    "    case = x.iloc[i]\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(user_dict[case['user_id']]) # User id encoding\n",
    "    x_value.append(1)\n",
    "    x_index.append(item_dict[case['movie_id']]) # Movie id encoding\n",
    "    x_value.append(1)\n",
    "    x_index.append(occ_dict[case['occupation']]) # Occupation id encoding\n",
    "    x_value.append(1)\n",
    "    x_index.append(gender_dict[case['sex']]) # Gender id encoding\n",
    "    x_value.append(1)\n",
    "    for j in genre:\n",
    "        if case[j] == 1: # 해당 장르가 1\n",
    "            x_index.append(genre_dict[j])\n",
    "            x_value.append(1)\n",
    "    x_index.append(age_index)\n",
    "    x_value.append((case['age'] - age_mean) / age_std)\n",
    "    data.append([x_index, x_value])\n",
    "    y.append(case['rating'] - w0)\n",
    "    if (i % 10000) == 0:\n",
    "        print('Encoding ', i, ' cases...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
    "\n",
    "class FM():\n",
    "    def __init__(self, N, K, data, y, alpha, beta, train_ratio=0.75, iterations=100, tolerance=0.005, l2_reg=True, verbose=True):\n",
    "        self.K = K          # Number of latent factors\n",
    "        self.N = N          # Number of x (variables)\n",
    "        self.n_cases = len(data)            # N of observations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # w 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
    "        # v 초기화\n",
    "        self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
    "        # Train/Test 분리\n",
    "        cutoff = int(train_ratio * len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.test_x = data[cutoff:]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.test_y = y[cutoff:]\n",
    "\n",
    "    def test(self):                                     # Training 하면서 RMSE 계산 \n",
    "        # SGD를 iterations 숫자만큼 수행\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            rmse1 = self.sgd(self.train_x, self.train_y)        # SGD & Train RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x, self.test_y)    # Test RMSE 계산     \n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                       # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance:  # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process\n",
    "    \n",
    "    # w, v 업데이트를 위한 Stochastic gradient descent \n",
    "    def sgd(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            x_idx = data[0]\n",
    "            x_0 = np.array(data[1])     # xi axis=0 [1, 2, 3]\n",
    "            x_1 = x_0.reshape(-1, 1)    # xi axis=1 [[1], [2], [3]]\n",
    "    \n",
    "            # biases\n",
    "            bias_score = np.sum(self.w[x_idx] * x_0)\n",
    "            \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1)          # v matrix * x\n",
    "            sum_vx = np.sum(vx, axis=0)         # sigma(vx)\n",
    "            sum_vx_2 = np.sum(vx * vx, axis=0)  # ( v matrix * x )의 제곱\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score\n",
    "            y_pred.append(y_hat)\n",
    "            error = y - y_hat\n",
    "            # w, v 업데이트\n",
    "            if self.l2_reg:     # regularization이 있는 경우\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "            else:               # regularization이 없는 경우\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1))\n",
    "        return RMSE(y_data, y_pred)\n",
    "    \n",
    "    def test_rmse(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data , y in zip(x_data, y_data):\n",
    "            y_hat = self.predict(data[0], data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE(y_data, y_pred)\n",
    "\n",
    "    def predict(self, idx, x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1, 1)\n",
    "\n",
    "        # biases\n",
    "        bias_score = np.sum(self.w[idx] * x_0)\n",
    "\n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx, axis=0)\n",
    "        sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 1.077456 ; Test RMSE = 1.080493\n",
      "Iteration: 20 ; Train RMSE = 1.053051 ; Test RMSE = 1.057796\n",
      "Iteration: 30 ; Train RMSE = 1.026664 ; Test RMSE = 1.033892\n",
      "Iteration: 40 ; Train RMSE = 0.999221 ; Test RMSE = 1.010252\n",
      "Iteration: 50 ; Train RMSE = 0.974847 ; Test RMSE = 0.990357\n",
      "Iteration: 60 ; Train RMSE = 0.955982 ; Test RMSE = 0.975819\n",
      "Iteration: 70 ; Train RMSE = 0.942098 ; Test RMSE = 0.965743\n",
      "Iteration: 80 ; Train RMSE = 0.931706 ; Test RMSE = 0.958702\n",
      "Iteration: 90 ; Train RMSE = 0.923697 ; Test RMSE = 0.953708\n",
      "Iteration: 100 ; Train RMSE = 0.917357 ; Test RMSE = 0.950123\n",
      "Iteration: 110 ; Train RMSE = 0.912208 ; Test RMSE = 0.947508\n",
      "Iteration: 120 ; Train RMSE = 0.907910 ; Test RMSE = 0.945558\n",
      "Iteration: 130 ; Train RMSE = 0.904221 ; Test RMSE = 0.944062\n",
      "Iteration: 140 ; Train RMSE = 0.900972 ; Test RMSE = 0.942881\n",
      "Iteration: 150 ; Train RMSE = 0.898041 ; Test RMSE = 0.941923\n",
      "Iteration: 160 ; Train RMSE = 0.895344 ; Test RMSE = 0.941126\n",
      "Iteration: 170 ; Train RMSE = 0.892821 ; Test RMSE = 0.940449\n",
      "Iteration: 180 ; Train RMSE = 0.890431 ; Test RMSE = 0.939867\n",
      "Iteration: 190 ; Train RMSE = 0.888148 ; Test RMSE = 0.939362\n",
      "Iteration: 200 ; Train RMSE = 0.885952 ; Test RMSE = 0.938925\n",
      "Iteration: 210 ; Train RMSE = 0.883833 ; Test RMSE = 0.938548\n",
      "Iteration: 220 ; Train RMSE = 0.881783 ; Test RMSE = 0.938228\n",
      "Iteration: 230 ; Train RMSE = 0.879798 ; Test RMSE = 0.937962\n",
      "Iteration: 240 ; Train RMSE = 0.877874 ; Test RMSE = 0.937747\n",
      "Iteration: 250 ; Train RMSE = 0.876011 ; Test RMSE = 0.937580\n",
      "Iteration: 260 ; Train RMSE = 0.874206 ; Test RMSE = 0.937460\n",
      "Iteration: 270 ; Train RMSE = 0.872457 ; Test RMSE = 0.937383\n",
      "Iteration: 280 ; Train RMSE = 0.870764 ; Test RMSE = 0.937346\n",
      "Iteration: 290 ; Train RMSE = 0.869122 ; Test RMSE = 0.937346\n",
      "Iteration: 300 ; Train RMSE = 0.867530 ; Test RMSE = 0.937380\n",
      "284 0.9373420652535662\n"
     ]
    }
   ],
   "source": [
    "K = 200\n",
    "fm1 = FM(num_x, K, data, y, alpha=0.00005, beta=0.002, train_ratio=0.75, iterations=300, tolerance=0.0001, l2_reg=True, verbose=True)\n",
    "result = fm1.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
